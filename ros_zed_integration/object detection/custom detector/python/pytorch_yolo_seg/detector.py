#!/usr/bin/env python3

import numpy as np

import argparse
import torch
import cv2
import os
import time
import pyzed.sl as sl
from ultralytics import YOLO
from ultralytics.engine.results import Results

from threading import Lock, Thread
from time import sleep
from typing import List

import ogl_viewer.viewer as gl
import cv_viewer.tracking_viewer as cv_viewer

if gl.GPU_ACCELERATION_AVAILABLE:
    import cupy as cp

# Globals
lock = Lock()
run_signal = False
exit_signal = False
image_net: np.ndarray = None
detections: List[sl.CustomMaskObjectData] = None
sl_mats: List[sl.Mat] = None  # We need it to keep the ownership of the sl.Mat
click_point = None

def on_mouse(event, x, y, flags, param):
    global click_point
    if event == cv2.EVENT_LBUTTONDOWN:
        click_point = (x, y)


def xywh2abcd_(xywh: np.ndarray) -> np.ndarray:
    output = np.zeros((4, 2))

    # Center / Width / Height -> BBox corners coordinates
    x_min = max(0, xywh[0] - 0.5 * xywh[2])
    x_max = (xywh[0] + 0.5 * xywh[2])
    y_min = max(0, xywh[1] - 0.5 * xywh[3])
    y_max = (xywh[1] + 0.5 * xywh[3])

    # A ------ B
    # | Object |
    # D ------ C

    output[0][0] = x_min
    output[0][1] = y_min

    output[1][0] = x_max
    output[1][1] = y_min

    output[2][0] = x_max
    output[2][1] = y_max

    output[3][0] = x_min
    output[3][1] = y_max
    return output


def detections_to_custom_masks_(dets: Results) -> List[sl.CustomMaskObjectData]:
    global sl_mats
    output = []
    sl_mats = []
    for det in dets.cpu().numpy():
        # Creating ingestable objects for the ZED SDK
        obj = sl.CustomMaskObjectData()

        # Bounding box
        box = det.boxes
        xywh = box.xywh[0]
        abcd = xywh2abcd_(xywh)

        obj.bounding_box_2d = abcd
        obj.label = box.cls
        obj.probability = box.conf
        obj.is_grounded = False

        # Mask
        if det.masks is not None:
            mask_bin = (det.masks.data[0] * 255).astype(np.uint8)
            mask_bin = mask_bin[int(abcd[0][1]): int(abcd[2][1]),
                                int(abcd[0][0]): int(abcd[2][0])]
            if not mask_bin.flags.c_contiguous:
                mask_bin = np.ascontiguousarray(mask_bin)

            # Mask as a sl mat
            sl_mat = sl.Mat(width=mask_bin.shape[1],
                            height=mask_bin.shape[0],
                            mat_type=sl.MAT_TYPE.U8_C1,
                            memory_type=sl.MEM.CPU)
            p_sl_as_cv = sl_mat.get_data()
            np.copyto(p_sl_as_cv, mask_bin)
            sl_mats.append(sl_mat)

            obj.box_mask = sl_mat
        else:
            print("[Warning] No mask found in the prediction. Did you use a seg model?")

        output.append(obj)

    return output


def torch_thread_(weights: str, img_size: int, conf_thres: float = 0.2, iou_thres: float = 0.45) -> None:
    global image_net, exit_signal, run_signal, detections

    print("Initializing Network...")
    model = YOLO(weights)
    print("Network Initialized...")

    while not exit_signal:
        if run_signal:
            lock.acquire()

            if gl.GPU_ACCELERATION_AVAILABLE:
                img_cupy = cp.asarray(image_net)[:, :, :3]  # Remove alpha channel on GPU
                # YOLO internally does some preprocessing, so we convert to numpy to let it handle the image
                # This is not the most efficient way, but it works for demonstration purposes.
                # It would be better to keep everything on GPU, with a zero-copy transfer like
                # `tensor = torch.from_dlpack(img_cupy.toDlpack())`
                img = cp.asnumpy(img_cupy)
            else:
                img = cv2.cvtColor(image_net, cv2.COLOR_RGBA2RGB)

            # Run inference
            det = model.predict(img, save=False, retina_masks=True, imgsz=img_size, conf=conf_thres, iou=iou_thres, verbose=False)[0]

            # ZED CustomMasks format
            detections = detections_to_custom_masks_(det)
            lock.release()
            run_signal = False

        sleep(0.01)


def main_(args: argparse.Namespace):
    global image_net, exit_signal, run_signal, detections, click_point

    # Determine memory type based on CuPy availability and user preference
    use_gpu = gl.GPU_ACCELERATION_AVAILABLE and not args.disable_gpu_data_transfer
    mem_type = sl.MEM.GPU if use_gpu else sl.MEM.CPU

    # Display memory type being used
    if use_gpu:
        print("ðŸš€ Using GPU data transfer with CuPy")

    capture_thread = Thread(target=torch_thread_,
                            kwargs={'weights': args.weights, 'img_size': args.img_size, "conf_thres": args.conf_thres})
    capture_thread.start()

    # Create a InitParameters object and set configuration parameters
    input_type = sl.InputType()
    if args.svo is not None:
        input_type.set_from_svo_file(args.svo)
    init_params = sl.InitParameters(input_t=input_type, svo_real_time_mode=True)
    init_params.coordinate_units = sl.UNIT.METER
    init_params.depth_mode = sl.DEPTH_MODE.NEURAL  # QUALITY
    init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP
    init_params.depth_maximum_distance = 50

    # Initialize the camera
    print("Initializing Camera...")
    zed = sl.Camera()
    status = zed.open(init_params)
    if status > sl.ERROR_CODE.SUCCESS:
        print(repr(status))
        exit()
    print("Camera Initialized")

    # Enable Positional Tracking
    positional_tracking_parameters = sl.PositionalTrackingParameters()
    # If the camera is static, uncomment the following line to have better performances and boxes sticked to the ground.
    # positional_tracking_parameters.set_as_static = True
    zed.enable_positional_tracking(positional_tracking_parameters)

    # Enable Object Detection
    obj_param = sl.ObjectDetectionParameters()
    obj_param.detection_model = sl.OBJECT_DETECTION_MODEL.CUSTOM_BOX_OBJECTS
    obj_param.enable_tracking = True
    obj_param.enable_segmentation = True
    zed.enable_object_detection(obj_param)

    # Display
    camera_infos = zed.get_camera_information()
    camera_res = camera_infos.camera_configuration.resolution

    # Create OpenGL viewer
    viewer = gl.GLViewer()
    point_cloud_res = sl.Resolution(min(camera_res.width, 720), min(camera_res.height, 404))
    point_cloud_render = sl.Mat()
    viewer.init(camera_infos.camera_model, point_cloud_res, obj_param.enable_tracking)
    point_cloud = sl.Mat(point_cloud_res.width, point_cloud_res.height, sl.MAT_TYPE.F32_C4, mem_type)
    image_left = sl.Mat(0, 0, sl.MAT_TYPE.U8_C4, mem_type)

    # Utilities for 2D display
    display_resolution = sl.Resolution(min(camera_res.width, 1280), min(camera_res.height, 720))
    image_scale = [display_resolution.width / camera_res.width, display_resolution.height / camera_res.height]
    image_left_ocv = np.full((display_resolution.height, display_resolution.width, 4), [245, 239, 239, 255], np.uint8)

    # Utilities for tracks view
    camera_config = camera_infos.camera_configuration
    tracks_resolution = sl.Resolution(400, display_resolution.height)
    track_view_generator = cv_viewer.TrackingViewer(tracks_resolution, camera_config.fps, init_params.depth_maximum_distance*1000, 1)
    track_view_generator.set_camera_calibration(camera_config.calibration_parameters)
    image_track_ocv = np.zeros((tracks_resolution.height, tracks_resolution.width, 4), np.uint8)

    masks_dir = "captured_masks"
    os.makedirs(masks_dir, exist_ok=True)

    cv2.namedWindow("ZED | 2D View and Birds View")
    cv2.setMouseCallback("ZED | 2D View and Birds View", on_mouse)

    # Prepare runtime retrieval
    runtime_params = sl.RuntimeParameters()
    obj_runtime_param = sl.CustomObjectDetectionRuntimeParameters()
    cam_w_pose = sl.Pose()
    image_left_tmp = sl.Mat(0, 0, sl.MAT_TYPE.U8_C4, mem_type)
    objects = sl.Objects()

    while viewer.is_available() and not exit_signal:
        if zed.grab(runtime_params) <= sl.ERROR_CODE.SUCCESS:
            # -- Get the image
            lock.acquire()
            zed.retrieve_image(image_left_tmp, sl.VIEW.LEFT, mem_type)
            image_net = image_left_tmp.get_data(memory_type=mem_type, deep_copy=False)
            lock.release()
            run_signal = True

            # -- Detection running on the other thread
            while run_signal:
                sleep(0.001)

            # Wait for detections
            lock.acquire()
            # -- Ingest detections
            zed.ingest_custom_mask_objects(detections)
            lock.release()
            zed.retrieve_custom_objects(objects, obj_runtime_param)

            # -- Display
            # Retrieve display data
            zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA, mem_type, point_cloud_res)
            zed.retrieve_image(image_left, sl.VIEW.LEFT, mem_type, display_resolution)
            zed.get_position(cam_w_pose, sl.REFERENCE_FRAME.WORLD)

            # 3D rendering
            viewer.updateData(point_cloud, objects)
            # 2D rendering
            if gl.GPU_ACCELERATION_AVAILABLE:
                image_left_ocv = cp.asnumpy(cp.asarray(image_left.get_data(memory_type=mem_type, deep_copy=False)))
            else:
                np.copyto(image_left_ocv, image_left.get_data(memory_type=mem_type, deep_copy=False))
            # Tracking view
            track_view_generator.generate_view(objects, image_left_ocv,image_scale ,cam_w_pose, image_track_ocv, objects.is_tracked)
            global_image = cv2.hconcat([image_left_ocv, image_track_ocv])
            cv2.imshow("ZED | 2D View and Birds View", global_image)

            if click_point is not None:
                cx, cy = click_point
                click_point = None
                if cx < image_left_ocv.shape[1]:
                    rx = camera_res.width / image_left_ocv.shape[1]
                    ry = camera_res.height / image_left_ocv.shape[0]
                    px = int(cx * rx)
                    py = int(cy * ry)

                    for obj in objects.object_list:
                        bbox = obj.bounding_box_2d
                        x_min = np.min(bbox[:, 0])
                        y_min = np.min(bbox[:, 1])

                        if x_min <= px <= np.max(bbox[:, 0]) and y_min <= py <= np.max(bbox[:, 1]):
                            if obj.mask.is_init():
                                mask_data = obj.mask.get_data()
                                mh, mw = mask_data.shape[:2]
                                lx, ly = int(px - x_min), int(py - y_min)
                                if 0 <= lx < mw and 0 <= ly < mh and mask_data[ly, lx] > 0:
                                    timestamp = int(time.time())
                                    fname = os.path.join(masks_dir, f"mask_{obj.raw_label}_{obj.id}_{timestamp}.png")
                                    cv2.imwrite(fname, mask_data)
                                    print(f"Saved mask: {fname}")

                                    # Retrieve and save point cloud details
                                    point_cloud_req = sl.Mat()
                                    if zed.retrieve_measure(point_cloud_req, sl.MEASURE.XYZRGBA, sl.MEM.CPU, camera_res) == sl.ERROR_CODE.SUCCESS:
                                        pc_data = point_cloud_req.get_data()
                                        
                                        # Crop to bounding box
                                        y1, y2 = int(y_min), int(np.max(bbox[:, 1]))
                                        x1, x2 = int(x_min), int(np.max(bbox[:, 0]))
                                        y1, x1 = max(0, y1), max(0, x1)
                                        y2, x2 = min(pc_data.shape[0], y2), min(pc_data.shape[1], x2)
                                        
                                        pc_crop = pc_data[y1:y2, x1:x2]
                                        
                                        # Resize mask to match crop (handling rounding errors)
                                        crop_h, crop_w = pc_crop.shape[:2]
                                        mask_resized = cv2.resize(mask_data, (crop_w, crop_h), interpolation=cv2.INTER_NEAREST) if (mask_data.shape[0] != crop_h or mask_data.shape[1] != crop_w) else mask_data
                                        
                                        # Filter valid points using the mask
                                        xyz_data = pc_crop[mask_resized > 0][:, :3]
                                        xyz_data = xyz_data[np.isfinite(xyz_data).all(axis=1)] # Remove NaNs/Infs
                                        
                                        pc_fname = os.path.join(masks_dir, f"cloud_{obj.raw_label}_{obj.id}_{timestamp}.npy")
                                        np.save(pc_fname, xyz_data)
                                        print(f"Saved point cloud: {pc_fname}")

            key = cv2.waitKey(10)
            if key in (27, ord('q'), ord('Q')):
                exit_signal = True
            if key == 105: #for 'i' key 
                track_view_generator.zoomIn()
            if key == 111 : #for 'o' key
                track_view_generator.zoomOut() 
        else:
            exit_signal = True

    viewer.exit()
    exit_signal = True
    zed.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, default='yolo11m-seg.pt', help='model.pt path')
    parser.add_argument('--svo', type=str, default=None, help='optional svo file')
    parser.add_argument('--img_size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf_thres', type=float, default=0.4, help='object confidence threshold')
    parser.add_argument('--disable-gpu-data-transfer', action='store_true', help='Disable GPU data transfer acceleration with CuPy even if CuPy is available')
    args = parser.parse_args()

    with torch.no_grad():
        main_(args)
